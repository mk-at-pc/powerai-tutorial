{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a proof-of-concept (POC) machine learning model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This training notebook provides\n",
    "- a low level introduction into the basic steps when setting up a POC machine learning model\n",
    "- an impression of the effort of individual steps\n",
    "- explains expressions frequently used like\n",
    "    - Model preprocessing including\n",
    "        - Transformer\n",
    "        - Transformer chain\n",
    "    - Model training\n",
    "        - Classifier\n",
    "    - Model evaluation\n",
    "        - Splitting for training and testing\n",
    "    - Model prediction (application)\n",
    "   \n",
    "## Your task\n",
    "[TASK] Fill in the \"...\" patterns with content!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Create a machine learning model capable of predicting the correct validity of a meter reading. The model will be user to take over decision-making currently applied by a user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "pandas.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### 1. Conception\n",
    "\n",
    "- Answer questions:\n",
    "    - How is the current process of validity check by the user?\n",
    "    - Which **data** does the user use to make the decision?\n",
    "    - Where do I find this **data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Acquire Data\n",
    "\n",
    "- Get access to DBs providing **data**\n",
    "- Write SQLs to access **data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here only single csv: In reality as set of DB resources\n",
    "data = pandas.read_csv(\"./data/readings.csv\", index_col=0) \\\n",
    "             .sort_values(by=\"readAt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyze data\n",
    "\n",
    "- What is the meaning of individual columns?\n",
    "- Columns suitable for decision-making? (e.g. too many na-values bad)\n",
    "- Is **data** assumed to be sufficient? If not, start over with **1. Conception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# E.g. check total number of valid / invalid readings, 1 or 0, respectively. \n",
    "# [TASK]: Check number of valid / invalid entries in data\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Data Aggregation Strategy\n",
    "\n",
    "How to group **data** belonging together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by contract, code and counter\n",
    "# [TASK]: Define grouper list based on items belonging together\n",
    "grouper = [...]\n",
    "select = [column for column in data if not column in grouper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = list(group[select] for context, group in data.groupby(grouper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = list(context for context, group in data.groupby(grouper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated[102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Structure and clean data\n",
    "\n",
    "- Structure **data** so you have a clear view how to clean it\n",
    "- Clean data: Remove insufficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose past readings from readings for assessment\n",
    "past = []\n",
    "assess = []\n",
    "for x in aggregated:\n",
    "    \n",
    "    # Last item of the row\n",
    "    assess_ = x.iloc[-1]\n",
    "    \n",
    "    # Append n-1 rows from group\n",
    "    past_ = x.iloc[0:-1]\n",
    "    \n",
    "    # [TASK] : Mask data not available @ assess_[\"createdAt\"]\n",
    "    # Hmm ... unfortunately if have to drop some values\n",
    "    # that have not been available @ decision making time\n",
    "    ...\n",
    "    \n",
    "    # -> Problem with DB updates! ...\n",
    "    \n",
    "    past.append(past_)\n",
    "    assess.append(assess_)\n",
    "    \n",
    "past[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # Features for predicting\n",
    "y = []\n",
    "\n",
    "select.remove(\"valid\")\n",
    "for past_, assess_ in zip(past, assess):\n",
    "    X.append(assess_[select].tolist() + past_[::-1].values.flatten().tolist())\n",
    "    y.append(assess_[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matrix shape of X: padding of individual # of past items\n",
    "n_features = 3 * (len(select) + 1) + len(select)\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Create training data\n",
    "\n",
    "- Decompose data into feature matrix X and target vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix needs to be 2D in this case. Since # of past readings varies,\n",
    "# some data points need to be dropped, some other need to be padded (with na)\n",
    "Xout = []\n",
    "for Xi in X:\n",
    "    \n",
    "    n = len(Xi)\n",
    "\n",
    "    # [TASK] : Modify elements in Xi such that list have n_features elements\n",
    "    ...\n",
    "    \n",
    "    Xout.append(Xi)\n",
    "\n",
    "# Feature matrix: Features characterizing the past reading history\n",
    "X = pandas.DataFrame(Xout)\n",
    "# Target vector: Binary vector (1 -> valid, 0 -> invalid)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparation for machine learning: Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conversion: Features must be floats. Think of how to convert\n",
    "    - dates\n",
    "    - strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn: Library containing a greate number of ML utilities\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion for filtering float-like columns from x\n",
    "def is_float_convertable(x):\n",
    "    \n",
    "    try: \n",
    "        x.astype(float)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Criterion for filtering date-like columns from x\n",
    "def is_datelike(x):\n",
    "    \n",
    "    try:\n",
    "        pandas.to_datetime(x)\n",
    "        \n",
    "        if not is_float_convertable(x):\n",
    "            return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, we have multi-type data available. All types have to be converted into float. \n",
    "# For converting categorical data, there are special encoding methodes available. \n",
    "\n",
    "# Decompose data by type\n",
    "numerical = [column for column in X if is_float_convertable(X[column])]\n",
    "dates = [column for column in X if is_datelike(X[column])]\n",
    "strings = [column for column in X if not column in numerical + dates]\n",
    "\n",
    "# [TASK] Convert dates to float: Total seconds since millenium\n",
    "null_date = datetime.datetime(2000, 1, 1)\n",
    "...\n",
    "    \n",
    "# Convert str columns: One-Hot-Encoding\n",
    "Xstr = X[strings].fillna(\"nan\")\n",
    "Xstr = pandas.DataFrame(OneHotEncoder(sparse=False) \\\n",
    "             .fit_transform(Xstr))\n",
    "X.drop(columns=strings, inplace=True)\n",
    "X.columns = range(len(X.columns))\n",
    "string_columns = np.arange(max(X.columns) + 1, (max(X.columns) + Xstr.shape[1] + 1))\n",
    "X[string_columns] = Xstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- na-fill strategy: Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = SimpleImputer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scaling: Normalize **data** features, such that each have similar impact, e.g. (-1, 1) normalization of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = RobustScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Training\n",
    "\n",
    "- Select suitable algorithm\n",
    "- Test if training technically works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn: Library containing a greate number of ML utilities\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Data splitting: Given subset of X, train to be able to predict associated subset y\n",
    "# [TASK] : Train using the first 2300 data points ...\n",
    "tree.fit(X[...], y[...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier prediction for data NOT used for training\n",
    "# [TASK] ... and predict target for the res\n",
    "pred = tree.predict(X[...])\n",
    "true = y[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measure quality of model: Precision and Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(pred, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, true, labels=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Systematic optimization\n",
    "\n",
    "- Algorithms have parameters to be choosen by user: Apply optimization\n",
    "- Split data systematically among different configurations and select the \"best\" model (requires definition of metric) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters taken by DecisionTree classifier\n",
    "params = {\"max_depth\" : [None, 5, 10, 20, 50],\n",
    "          \"min_samples_split\" : [2, 5, 10],\n",
    "          \"max_features\" : [\"auto\", \"sqrt\", \"log2\"]}\n",
    "\n",
    "# Create data split strategy\n",
    "cv = KFold(5, random_state=42, shuffle=True)\n",
    "\n",
    "# Init grid search for optimum parameters\n",
    "grd = GridSearchCV(tree, params, cv=cv, scoring=\"precision\")\n",
    "\n",
    "# Train on all possible combinations of parameters\n",
    "grd.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best classifier\n",
    "clf = grd.best_estimator_\n",
    "\n",
    "# Total precision\n",
    "grd.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total score\n",
    "confusion = []\n",
    "for train, test in cv.split(X): # Provides arrays of indices\n",
    "    \n",
    "    # [TASK] : \"fit clf\" using train and \"pred\" using test indices\n",
    "    clf.fit(X[...], y[...])\n",
    "    pred = clf.predict(X[...])\n",
    "    confusion.append(confusion_matrix(pred, y[...], labels=[1, 0]))\n",
    "    \n",
    "confusion = np.array(confusion).sum(axis=0)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Bring to application\n",
    "\n",
    "- Transformer implement: Implement custom preprocessing into transformer class object\n",
    "- Transformer chain: Chain all processing and classification items together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer object: Steps 4. and 5. in one class \n",
    "from utils import CustomPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer chain\n",
    "chain = Pipeline([(\"custom\", CustomPreprocessing()),\n",
    "                  (\"fillna\", SimpleImputer()),\n",
    "                  (\"scale\", RobustScaler()),\n",
    "                  (\"clf\", DecisionTreeClassifier(**clf.get_params()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on complete data set\n",
    "chain.fit(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data from application:\n",
    "file = 1 # Choose from 1 or 2\n",
    "# Two counters belonging to a single contract\n",
    "appl_data = pandas.read_csv(f\"./data/readings_application_{file}.csv\", index_col=0) \\\n",
    "             .sort_values(by=\"readAt\") \\\n",
    "            [data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validity of readings with valid == nan\n",
    "# [TASK] : Apply chain.predict on appl_data\n",
    "pred = ...\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
